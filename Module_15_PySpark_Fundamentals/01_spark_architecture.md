# Apache Spark Architecture - Complete Guide

## ğŸ“š What You'll Learn
- Spark ecosystem and components
- Cluster architecture and execution model
- RDDs, DataFrames, and Datasets
- Lazy evaluation and DAGs
- Spark memory management
- Interview preparation

**Duration**: 2 hours  
**Difficulty**: â­â­â­ Intermediate

---

## ğŸ¯ What is Apache Spark?

### Definition
**Apache Spark** is a unified analytics engine for large-scale data processing. It provides:
- In-memory computing (100x faster than Hadoop MapReduce)
- Support for batch and streaming data
- Machine learning and graph processing
- SQL queries on distributed data

### The Spark Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SPARK APPLICATIONS                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Spark SQL  â”‚ Spark       â”‚   MLlib     â”‚  GraphX     â”‚Structuredâ”‚
â”‚  DataFrames â”‚ Streaming   â”‚   ML        â”‚  Graph      â”‚Streaming â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                        SPARK CORE (RDD API)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    CLUSTER MANAGERS                              â”‚
â”‚         Standalone  |  YARN  |  Mesos  |  Kubernetes             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    DATA SOURCES                                  â”‚
â”‚    HDFS  |  S3  |  Cassandra  |  JDBC  |  Kafka  |  Files        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Cluster Architecture

### Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           DRIVER PROGRAM                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                      SparkContext/SparkSession                    â”‚  â”‚
â”‚  â”‚  - Creates RDDs/DataFrames                                        â”‚  â”‚
â”‚  â”‚  - Builds DAG of operations                                       â”‚  â”‚
â”‚  â”‚  - Schedules tasks                                                â”‚  â”‚
â”‚  â”‚  - Coordinates with cluster manager                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLUSTER MANAGER                                 â”‚
â”‚              (YARN / Mesos / Kubernetes / Standalone)                   â”‚
â”‚                   - Allocates resources                                 â”‚
â”‚                   - Manages worker nodes                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                               â”‚
                    â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        WORKER NODE 1        â”‚   â”‚        WORKER NODE 2        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚      EXECUTOR 1       â”‚  â”‚   â”‚  â”‚      EXECUTOR 2       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”     â”‚  â”‚   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”     â”‚  â”‚
â”‚  â”‚  â”‚Task1â”‚ â”‚Task2â”‚     â”‚  â”‚   â”‚  â”‚  â”‚Task3â”‚ â”‚Task4â”‚     â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜     â”‚  â”‚   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜     â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚   CACHE         â”‚ â”‚  â”‚   â”‚  â”‚  â”‚   CACHE         â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Roles

| Component | Role |
|-----------|------|
| **Driver** | Main program, creates SparkSession, orchestrates execution |
| **Cluster Manager** | Allocates resources across cluster |
| **Worker Node** | Physical/virtual machine running executors |
| **Executor** | JVM process running tasks and storing data |
| **Task** | Smallest unit of work on a partition |

---

## ğŸ“Š Data Abstractions

### Evolution of Spark APIs

```
Spark 1.0          Spark 1.3         Spark 1.6         Spark 2.0+
   â”‚                   â”‚                 â”‚                  â”‚
   â–¼                   â–¼                 â–¼                  â–¼
 â”Œâ”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ RDD â”‚   â†’      â”‚DataFrameâ”‚   â†’   â”‚ Dataset â”‚   â†’    â”‚Unified  â”‚
 â””â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚DataFrameâ”‚
 Low-level         Optimized         Type-safe         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Functional        SQL-like          Scala/Java         Best of
                                                        both
```

### RDD (Resilient Distributed Dataset)

```python
# Low-level API - full control but less optimization
rdd = sc.parallelize([1, 2, 3, 4, 5])
result = rdd.map(lambda x: x * 2).filter(lambda x: x > 4).collect()

# Key characteristics:
# - Immutable
# - Partitioned across cluster
# - Fault-tolerant (can rebuild from lineage)
# - Lazy evaluation
```

### DataFrame

```python
# High-level API - SQL-like, optimized by Catalyst
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Example").getOrCreate()

df = spark.read.csv("data.csv", header=True, inferSchema=True)
result = df.filter(df.age > 30).select("name", "age").orderBy("age")

# Key characteristics:
# - Structured (schema)
# - Optimized by Catalyst optimizer
# - Can use SQL syntax
# - Less control but better performance
```

### When to Use What?

| Use Case | Best Choice |
|----------|-------------|
| Structured data with known schema | DataFrame/Dataset |
| SQL-like operations | DataFrame |
| Need full control over data | RDD |
| Machine learning pipelines | DataFrame |
| Graph processing | GraphX (RDD-based) |
| Streaming | Structured Streaming (DataFrame) |

---

## âš¡ Lazy Evaluation

### What is Lazy Evaluation?

Spark doesn't execute operations immediately. It builds a plan (DAG) and executes only when an **action** is called.

```python
# These are TRANSFORMATIONS (lazy - nothing happens yet)
df1 = spark.read.csv("data.csv")          # Just a plan
df2 = df1.filter(df1.age > 30)            # Still just a plan
df3 = df2.select("name", "age")           # Still just a plan
df4 = df3.groupBy("age").count()          # Still just a plan

# This is an ACTION (triggers execution!)
result = df4.show()  # NOW Spark executes everything!
```

### Transformations vs Actions

| Transformations (Lazy) | Actions (Trigger Execution) |
|------------------------|----------------------------|
| `filter()`, `select()` | `show()`, `collect()` |
| `map()`, `flatMap()` | `count()`, `take()` |
| `groupBy()`, `orderBy()` | `write()`, `save()` |
| `join()`, `union()` | `first()`, `head()` |
| `distinct()`, `sample()` | `foreach()`, `reduce()` |

### Why Lazy Evaluation?

1. **Optimization**: Spark can optimize the entire plan before executing
2. **Efficiency**: Combines operations to minimize data movement
3. **Fault Tolerance**: Can rebuild from lineage if partition fails

---

## ğŸ“ˆ DAG (Directed Acyclic Graph)

### What is DAG?

Spark creates a DAG of operations before execution:

```
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ read()  â”‚
                     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                          â”‚
                     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                     â”‚filter() â”‚
                     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                          â”‚
                     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                     â”‚select() â”‚
                     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                          â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                               â”‚
     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
     â”‚groupBy()â”‚                     â”‚ join()  â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â”‚                               â”‚
     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
     â”‚ count() â”‚                     â”‚orderBy()â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â”‚                               â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
                     â”‚ show()  â”‚  â† ACTION triggers execution
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Jobs, Stages, and Tasks

```
JOB (triggered by action)
â”‚
â”œâ”€â”€ STAGE 1 (narrow transformations - no shuffle)
â”‚   â”œâ”€â”€ Task 1 (partition 1)
â”‚   â”œâ”€â”€ Task 2 (partition 2)
â”‚   â””â”€â”€ Task 3 (partition 3)
â”‚
â”œâ”€â”€ [SHUFFLE] â† Stage boundary (data exchange between partitions)
â”‚
â””â”€â”€ STAGE 2 (after shuffle)
    â”œâ”€â”€ Task 1 (partition 1)
    â”œâ”€â”€ Task 2 (partition 2)
    â””â”€â”€ Task 3 (partition 3)
```

### Narrow vs Wide Transformations

| Narrow (No Shuffle) | Wide (Shuffle Required) |
|--------------------|------------------------|
| `map()`, `filter()` | `groupBy()`, `reduceByKey()` |
| `flatMap()`, `union()` | `join()`, `repartition()` |
| Fast, parallel | Slow, network I/O |

---

## ğŸ’¾ Memory Management

### Spark Memory Areas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     EXECUTOR MEMORY                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              SPARK MEMORY (spark.memory.fraction)         â”‚  â”‚
â”‚  â”‚                        (60% default)                      â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”‚
â”‚  â”‚  â”‚   EXECUTION MEMORY  â”‚  â”‚      STORAGE MEMORY         â”‚â”‚  â”‚
â”‚  â”‚  â”‚   (shuffles, joins, â”‚  â”‚   (cached RDDs/DataFrames)  â”‚â”‚  â”‚
â”‚  â”‚  â”‚    sorts, aggs)     â”‚  â”‚                             â”‚â”‚  â”‚
â”‚  â”‚  â”‚        50%          â”‚  â”‚          50%                â”‚â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                  USER MEMORY (40%)                        â”‚  â”‚
â”‚  â”‚            (user data structures, UDFs)                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Memory Configurations

| Configuration | Description | Default |
|--------------|-------------|---------|
| `spark.executor.memory` | Total executor memory | 1g |
| `spark.memory.fraction` | Fraction for Spark operations | 0.6 |
| `spark.memory.storageFraction` | Fraction of Spark memory for storage | 0.5 |
| `spark.driver.memory` | Driver memory | 1g |

---

## ğŸ”§ SparkSession Configuration

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("MyApp") \
    .master("local[*]") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "2g") \
    .config("spark.sql.shuffle.partitions", "200") \
    .config("spark.default.parallelism", "100") \
    .getOrCreate()

# Common configurations
# spark.executor.instances = number of executors
# spark.executor.cores = cores per executor
# spark.sql.shuffle.partitions = partitions after shuffle
# spark.default.parallelism = default RDD partitions
```

---

## ğŸ“ Interview Questions

### Q1: What is Apache Spark and how is it different from Hadoop MapReduce?
**A:** Spark is a distributed computing framework that:
- Processes data **in-memory** (100x faster than MapReduce for iterative algorithms)
- Supports **lazy evaluation** with DAG optimization
- Has **unified API** for batch, streaming, ML, and graph processing
- Provides **interactive queries** via Spark SQL

MapReduce writes intermediate results to disk, making it slower for iterative operations.

### Q2: Explain the Spark architecture.
**A:**
1. **Driver**: Main program that creates SparkSession, builds DAG, schedules tasks
2. **Cluster Manager**: Allocates resources (YARN, Mesos, K8s, Standalone)
3. **Workers**: Physical machines running executors
4. **Executors**: JVM processes that run tasks and cache data
5. **Tasks**: Smallest unit of work operating on partitions

### Q3: What is lazy evaluation in Spark?
**A:** Spark doesn't execute transformations immediately. It builds a DAG of operations and only executes when an action is called. Benefits:
- Allows optimization across entire plan
- Minimizes data movement
- Enables fault tolerance through lineage

### Q4: What is the difference between transformation and action?
**A:**
- **Transformation**: Creates new RDD/DataFrame from existing one (lazy, returns new dataset). Examples: map, filter, groupBy
- **Action**: Triggers computation and returns result to driver (eager). Examples: count, collect, show, write

### Q5: What is DAG in Spark?
**A:** Directed Acyclic Graph represents the sequence of computations on data. Spark builds a DAG of transformations and optimizes it before execution. DAG is divided into stages based on shuffle boundaries.

### Q6: What is the difference between narrow and wide transformations?
**A:**
- **Narrow**: Data stays on same partition, no shuffle (filter, map). Fast, parallelizable.
- **Wide**: Data moves between partitions, requires shuffle (groupBy, join). Creates stage boundary.

### Q7: What is RDD?
**A:** Resilient Distributed Dataset - the fundamental data structure in Spark:
- **Resilient**: Fault-tolerant, can rebuild from lineage
- **Distributed**: Partitioned across cluster nodes
- **Dataset**: Collection of records

### Q8: DataFrame vs RDD - when to use which?
**A:**
- **DataFrame**: Structured data, SQL-like operations, Catalyst optimization. Use for most cases.
- **RDD**: Unstructured data, fine-grained control, custom serialization. Use when DataFrame doesn't fit.

### Q9: What is a shuffle in Spark?
**A:** Shuffle is redistribution of data across partitions, required for wide transformations like groupBy, join, reduceByKey. It involves:
- Writing to disk
- Network I/O between nodes
- Creates stage boundary

Shuffles are expensive - minimize them for better performance.

### Q10: How does Spark achieve fault tolerance?
**A:** Through lineage tracking:
1. Spark records all transformations as a DAG
2. If a partition is lost, Spark replays transformations from source
3. Checkpointing can save state to avoid long lineage recomputation

---

## ğŸ”— Related Topics
- [PySpark DataFrames â†’](./02_pyspark_dataframes.md)
- [Transformations and Actions â†’](./03_transformations_actions.md)
- [Performance Optimization â†’](./04_performance.md)

---

*Next: Learn about PySpark DataFrames*
